model:
  name: gpt2
  epochs: 3
  batch_size: 2
  learning_rate: 0.0001

lora:
  rank: 8
  alpha: 16
  dropout: 0.1

training:
  output_dir: ./checkpoints
  checkpoint_interval: 5

dataset:
  path: ./train.json
